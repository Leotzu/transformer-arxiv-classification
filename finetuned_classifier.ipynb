{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOgFjAUZVMWTacnpe28miCj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Leotzu/transformer-arxiv-classification/blob/main/finetuned_classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** This notebook assumes that you have already trained a base model by running arxiv_transformer.ipynb"
      ],
      "metadata": {
        "id": "aF2N-ik4Akia"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 1)** Setup environment and give your notebook access to google drive (which is where your saved model from before should be)"
      ],
      "metadata": {
        "id": "VlJvNyp9AqjH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "htGAOjdELmgz"
      },
      "outputs": [],
      "source": [
        "!pip install torch pandas numpy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mount drive to access json file and save/load models and vocab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "A0k85InmVgVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "PHiuEiOrLx1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Step 2)** Define your model and functions for data loading and preprocessing\n",
        "\n",
        "- In Config, make sure to change the *project_dir*, *data_path*, *vocab_path*, and *pretrained_model_file_path* to where you have this project in your drive, where you loaded the *finetune_train.jsonl* data, where you saved the vocab file when training the base model, and where the base model you're wishing to finetune is.\n",
        "\n",
        "- In Config, change *prefix* to differentiate this training run from any others you do (it will be added to the beginning of every model and checkpoint saved during preprocessing and training)\n",
        "\n",
        "- Also be sure to have the same model hyperparameters in Config as you had for the original model you're finetuning."
      ],
      "metadata": {
        "id": "XW3elGJQBT5z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Config:\n",
        "    # project directories\n",
        "    project_dir = '/content/drive/MyDrive/your_project_path'\n",
        "    data_path = project_dir + '/data/finetune_train.jsonl'\n",
        "    models_path = project_dir + '/models'\n",
        "    vocab_file_path = project_dir + '/vocab/50k_vocab.pkl'\n",
        "    pretrained_model_file_path = models_path + '50k_model_epoch_30.pth'\n",
        "    prefix = 'finetune'\n",
        "\n",
        "    # hyperparameters (must be same as original model)\n",
        "    d_model = 256\n",
        "    nhead = 8\n",
        "    num_encoder_layers = 3\n",
        "    num_decoder_layers = 3\n",
        "    dim_feedforward = 1024\n",
        "    max_seq_length = 256\n",
        "    dropout_rate = 0.3\n",
        "\n",
        "    # hyperparameters for finetuning (these you can experiment with)\n",
        "    num_epochs = 10\n",
        "    learning_rate = 0.001\n",
        "    batch_size = 32"
      ],
      "metadata": {
        "id": "TqQ9AedxQMyG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** Be sure to use exact Vocabulary class from arXiv_transformer.ipynb"
      ],
      "metadata": {
        "id": "PxqH6ulZD8mC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_vocab(vocab_path):\n",
        "    with open(Config.vocab_file_path, 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "    return vocab\n",
        "\n",
        "# Use EXACT Vocabulary class from original model notebook:\n",
        "class Vocabulary:\n",
        "    def __init__(self):\n",
        "        self.stoi = {\"<pad>\": 0, \"<unk>\": 1, \"<eos>\": 2}\n",
        "        self.itos = {0: \"<pad>\", 1: \"<unk>\", 2: \"<eos>\"}\n",
        "\n",
        "    def build_vocab(self, texts, min_freq=2):\n",
        "        counter = {}\n",
        "        for text in texts:\n",
        "            for word in text.split():\n",
        "                if word not in counter:\n",
        "                    counter[word] = 0\n",
        "                counter[word] += 1\n",
        "        idx = len(self.stoi)\n",
        "        for word, count in counter.items():\n",
        "            if count >= min_freq:\n",
        "                self.stoi[word] = idx\n",
        "                self.itos[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "class ArxivDataset(Dataset):\n",
        "    def __init__(self, data, vocab):\n",
        "        self.vocab = vocab\n",
        "        # note: in this jsonl, the abstracts are called 'text' and labels are called 'label'\n",
        "        self.data = [self.vectorize(text['text']) for text in data]\n",
        "        # Convert boolean strings to integers; True becomes 1, False becomes 0\n",
        "        self.labels = torch.tensor([1 if text['label'] == 'True' else 0 for text in data])\n",
        "\n",
        "    def vectorize(self, text):\n",
        "        tokens = [self.vocab.stoi.get(word, self.vocab.stoi['<unk>']) for word in text.split()]\n",
        "        tokens.append(self.vocab.stoi['<eos>'])\n",
        "        if len(tokens) > Config.max_seq_length:\n",
        "            tokens = tokens[:Config.max_seq_length]\n",
        "        else:\n",
        "            tokens += [self.vocab.stoi['<pad>']] * (Config.max_seq_length - len(tokens))\n",
        "        return torch.tensor(tokens, dtype=torch.long)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx], self.labels[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "def load_data(file_path, split_ratio=0.8):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        data = [json.loads(line) for line in file]\n",
        "    np.random.shuffle(data)\n",
        "    split_idx = int(len(data) * split_ratio)\n",
        "    return data[:split_idx], data[split_idx:]\n",
        "\n",
        "def get_data():\n",
        "    train_data, test_data = load_data(Config.data_path)\n",
        "    vocab = load_vocab(Config.vocab_file_path)  # Load saved vocab from original model training\n",
        "    train_dataset = ArxivDataset(train_data, vocab)\n",
        "    test_dataset = ArxivDataset(test_data, vocab)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=Config.batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=Config.batch_size, shuffle=False)\n",
        "    return train_loader, test_loader, vocab"
      ],
      "metadata": {
        "id": "pkNvkMg4QMwV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerClassifier(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super(TransformerClassifier, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, Config.d_model)\n",
        "        self.transformer = nn.TransformerEncoder(\n",
        "            nn.TransformerEncoderLayer(\n",
        "                d_model=Config.d_model, nhead=Config.nhead,\n",
        "                dim_feedforward=Config.dim_feedforward, dropout=Config.dropout_rate\n",
        "            ), num_layers=Config.num_encoder_layers\n",
        "        )\n",
        "        self.fc = nn.Linear(Config.d_model, 2)\n",
        "\n",
        "    def forward(self, src):\n",
        "        src = self.embedding(src) * np.sqrt(Config.d_model)\n",
        "        src = src.permute(1, 0, 2)  # Transformer expects [seq_len, batch_size, d_model]\n",
        "        output = self.transformer(src)\n",
        "        output = output.mean(dim=0)\n",
        "        return self.fc(output)"
      ],
      "metadata": {
        "id": "zlC1brSiQMtj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 3)** Train the model\n",
        "\n",
        "- This function will save your finetuned models to model_path after each epoch."
      ],
      "metadata": {
        "id": "TFIBHoUXET0F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train():\n",
        "    train_loader, test_loader, vocab = get_data()\n",
        "    model = TransformerClassifier(len(vocab.stoi)).to('cuda')\n",
        "    optimizer = optim.Adam(model.parameters(), lr=Config.learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print('finetuning started...')\n",
        "    for epoch in range(Config.num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        for inputs, labels in tqdm(train_loader):\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_loader)\n",
        "        print(f'Epoch {epoch + 1} Training Loss: {avg_train_loss:.4f}')\n",
        "\n",
        "        model.eval()\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in test_loader:\n",
        "                inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                total_loss += loss.item()\n",
        "                predictions = outputs.argmax(dim=1)\n",
        "                correct_predictions += (predictions == labels).sum().item()\n",
        "\n",
        "        avg_test_loss = total_loss / len(test_loader)\n",
        "        accuracy = correct_predictions / len(test_loader.dataset)\n",
        "        print(f'Epoch {epoch + 1} Test Loss: {avg_test_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "        # Optional: Save the model\n",
        "        torch.save(model.state_dict(), f'{Config.models_path}/{Config.prefix}_model_epoch_{epoch + 1}.pth')\n",
        "\n",
        "train()"
      ],
      "metadata": {
        "id": "oj1RwrnMQy3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Step 4)** Evaluate your finetuned model and perform inference in predict_ai_relevance() to see determine whether a custom text prompt is AI-relevant or not."
      ],
      "metadata": {
        "id": "1LQr-1UtEjdt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate():\n",
        "    # load the vocab and test data\n",
        "    train_loader, test_loader, vocab = get_data()\n",
        "\n",
        "    # load the model and evaluate on test data\n",
        "    model = TransformerClassifier(len(vocab.stoi)).to('cuda')\n",
        "    model.load_state_dict(torch.load(f'{Config.models_path}/{Config.prefix}_model_epoch_{Config.num_epochs}.pth'))\n",
        "    model.eval()\n",
        "\n",
        "    # use cross entropy loss criterion\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    total_loss = 0\n",
        "    correct_predictions = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "            predictions = outputs.argmax(dim=1)\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "\n",
        "    avg_test_loss = total_loss / len(test_loader)\n",
        "    accuracy = correct_predictions / len(test_loader.dataset)\n",
        "    print(f'Final Test Loss: {avg_test_loss:.4f}, Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "evaluate()"
      ],
      "metadata": {
        "id": "LMoGBx8VQy1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_ai_relevance(prompt):\n",
        "    # Load the vocabulary\n",
        "    vocab_path = Config.vocab_file_path\n",
        "    with open(vocab_path, 'rb') as f:\n",
        "        vocab = pickle.load(f)\n",
        "\n",
        "    # load the model\n",
        "    model = TransformerClassifier(len(vocab.stoi)).to('cuda')\n",
        "    model_path = f'{Config.models_path}/{Config.prefix}_model_epoch_{Config.num_epochs}.pth'\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "\n",
        "    # process the prompt\n",
        "    tokens = [vocab.stoi.get(word, vocab.stoi['<unk>']) for word in prompt.split()]\n",
        "    tokens.append(vocab.stoi['<eos>'])\n",
        "    if len(tokens) > Config.max_seq_length:\n",
        "        tokens = tokens[:Config.max_seq_length]\n",
        "    else:\n",
        "        tokens += [vocab.stoi['<pad>']] * (Config.max_seq_length - len(tokens))\n",
        "    input_tensor = torch.tensor([tokens], dtype=torch.long).to('cuda')\n",
        "\n",
        "    # make prediction\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "        predictions = torch.softmax(outputs, dim=1)\n",
        "        predicted_class = predictions.argmax(dim=1).item()\n",
        "\n",
        "    # interpret prediction\n",
        "    ai_related = \"Yes\" if predicted_class == 1 else \"No\"\n",
        "    confidence = predictions[0, predicted_class].item()\n",
        "    return f\"AI-related: {ai_related}\\nPrediction Confidence: {confidence:.4f}\"\n",
        "\n",
        "\n",
        "prompt = \"In this study we study the various morphology of mitochondria in axons, using shape descriptors as a\" # Change this to any prompt of your choosing\n",
        "result = predict_ai_relevance(prompt)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "aN-Fpk1_Rzbw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}